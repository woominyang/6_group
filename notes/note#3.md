# note#3

**Q1. 경사하강법 이론**

수인: 

경사 하강법(GD)은 최적화 알고리즘으로, 비용 함수를 최소화하기 위해 파라미터를 조금씩 조정하며 최적의 해법을 찾는 방법이라고 배웠어요

3가지 종류로 배치 경사 하강법, 확률적 경사 하강법, 그리고 미니배치 경사 하강법 이렇게 나뉘어요.

근데  미니배치 경사 하강법이 두가지의 장점을 조합한 방법이면, 미니배치 경사 하강법만 사용해도 되지 않을까요??

우민:

 배치 경사 하강법은 모든 훈련 데이터 세트를 각 반복마다 처리하는 방법으로, 한 번의 iteration에서 모든 데이터 샘플의 기울기를 계산하고 모든 매개변수에 대해 업데이트합니다. 이는 최적의 해법을 찾기까지 아주 오랜 시간이 걸립니다.
 확률적 경사 하강법은 전체 데이터 세트를 랜덤하게 섞고, 한 번의 iteration에서 하나의 데이터에 대하여 기울기를 계산하고 업데이트하는 방법입니다. 배치 경사 하강법과 마찬가지로 최적의 해법을 찾을 때까지 계산을 반복하죠.
 미니 배치 경사 하강법은 위의 두 방법보다 더 빠르게 동작합니다. 모든 훈련 데이터 세트를 여러 개의 mini batch들로 나눈 뒤, 한 번의 iteration에서 하나의 미니 배치에 속한 데이터들에 대해 기울기를 구한 후, 그것들의 평균 기울기를 통해 업데이트하는 방식으로 작동합니다.

 수인님이 말씀해주신 미니 배치 경사 하강법은 다른 두 방식보다 빠르고, 전역 최솟값에 더 가까이 도달할 수 있지만 지역 최솟값은 빠져나오기 힘들 수 있다는 단점이 있습니다.

 각각의 방법에는 장점도 있고, 단점도 있어서 학습할 데이터 세트에 따라 적절한 방법을 찾아 사용하는 게 좋아 보이네요.

---

**Q2. 확률적 경사 하강법 실습**

**SGD 알고리즘 코드 구현 설명하기**

```
n_epochs = 50
t0, t1 = 5, 50  # 학습 스케줄 하이퍼파라미터

def learning_schedule(t):
    return t0 / (t + t1)

theta = np.random.randn(2,1)  # 랜덤 초기화

for epoch in range(n_epochs):
    for i in range(m):
        if epoch == 0 and i < 20:                    # 책에는 없음
            y_predict = X_new_b.dot(theta)           # 책에는 없음
            style = "b-" if i > 0 else "r--"         # 책에는 없음
            plt.plot(X_new, y_predict, style)        # 책에는 없음
        random_index = np.random.randint(m)
        xi = X_b[random_index:random_index+1]
        yi = y[random_index:random_index+1]
        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)
        eta = learning_schedule(epoch * m + i)
        theta = theta - eta * gradients
        theta_path_sgd.append(theta)
```

     

수인: 이 코드에서는 SGD를 이용해 theta가 업데이트 되네요.
그레이디언트는 하나의 랜덤한 훈련 데이터 포인트(xi, yi)를 사용하여 계산되고,
theta가 조금씩 조정되는거 같아요.

우민: 

 여기서 그래디언트는 비용함수의 그래디언트를 계산하는 거겠죠?

 선형 회귀의 목적은 비용함수를 줄이는 것이 목적이니, 이 코드에서는 평균 제곱 오차를 최소화하기 위한 그래디언트를 계산하는 것으로 봐도 괜찮을 것 같아요.

 theta는 보통 파라미터를 나타내는 변수로 사용하니까 여기서는 파라미터인 theta를 조정하여 예측값을 실제 값에 최대한 가깝게 만들도록 모델의 파라미터를 업데이트할 때 사용되겠네요.

---

**Q3. epoch=100, Dataset=1000, 미니 배치 크기=50 일때, 업데이트 되는 횟수는?**  

**[미니배치 경사 하강법]**

수인: 제 생각에는 1000개의 데이터를 50 개씩 묶어서 업데이트를 진행하면 20번의 미니 배치 업데이트가 이루어질꺼같아요.

우민: 하나의 epoch에서 업데이트 횟수는 (총 데이터 세트 크기) / (미니 배치 크기) 이므로, 한 epoch에서 20번의 업데이트가 발생하고 총 100개의 epoch가 있기 때문에 전체 업데이트 횟수는 2000번 이라고 생각해요.

---

**Q4. 과대 적합을 감소시키기 위해 규제를 사용하는 것은 모델 성능을 향상 시키는 중요한 방법 중 하나 입니다. 여기서 릿지 회귀와 라쏘 회귀, 그리고 엘라스틱넷을 비교해보면 어떤 차이점과 장단점을 가질까요?**

수인:

선형 회귀 모델은 데이터를 가장 잘 설명하는 직선을 찾으려고 노력하는데, 때로는 데이터에 너무 딱 맞게 학습하면 문제가 발생한다고 해요.
릿지 회귀는 가중치를 작게 만들어서 모든 특성을 조금씩 고려하도록 돕고,
라쏘 회귀는 가중치를 0으로 만들어서 어떤 특성은 아예 고려하지 않도록 합니다.
엘라스틱넷에 대해 간단히 설명해주실 수 있나요?

우민:

 엘라스틱넷 회귀는 릿지 회귀와 라쏘 회귀의 하이브리드 모델이라고 생각하면 될 것 같아요.

 릿지 회귀에서 가중치를 작게 만든다는 것은 알파 값을 설정하고 과적합 하게 커진 회귀 계수를 줄이는 규제를 말하고,

 라쏘 회귀에서 가중치를 0으로 만든다는 것은 덜 중요한 특성의 가중치를 제외하는 거예요.

 엘라스틱넷 회귀는 릿지 회귀와 라쏘 회귀의 규제 방식을 둘 다 사용하는데, 필요 없는 피처를 제외하는 라쏘 회귀 방식을 사용하면 회귀 계수가 급격히 변화합니다. 이때 릿지 회귀의 규제를 추가시켜 회귀 계수를 적절히 유지하는 방법으로 사용할 수 있어요.