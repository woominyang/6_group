# HW#3

2023.10.31

**Q1. 기계 학습에서 학습이란 무엇인지를 정리하시오(2점).**

**( 가중치, 손실함수가 무엇인지를 정리하고, 데이터, 가중치, 손실함수를 이용하여 학습이 무엇인지를 정리함.)**

기계학습에서 학습은 모델이 훈련 데이터를 기반으로 가중치 매개변수를 조정하여 최적의 값을 찾는 과정입니다.

학습의 시작점은 데이터입니다.
이 데이터는 훈련 데이터로 나누어집니다.
훈련 데이터에는 입력 특성과 그에 상응하는 정확한 출력 레이블이 함께 제공됩니다.
이 데이터를 사용하여 모델을 훈련시킵니다.

모델은 입력 특성에 대한 가중치를 사용하여 예측을 수행합니다.
이 가중치는 초기에는 임의의 값으로 시작하며, 학습 과정 중에 최적의 값으로 조정됩니다.
가중치를 조절하면 모델의 예측이 향상됩니다.

손실 함수는 모델의 예측과 실제 레이블 간의 차이를 측정하는 함수입니다.
이 함수를 통해 모델의 성능을 평가하고 얼마나 잘 예측하는지를 판단합니다.
일반적으로는 평균 제곱 오차나 다른 손실 함수를 사용하여 예측 오차를 측정합니다.

모델은 가중치를 조정하여 손실 함수의 값을 최소화하는 방향으로 학습합니다.
이를 위해 경사 하강법과 같은 최적화 알고리즘을 사용합니다.
손실 함수의 기울기를 계산하고, 이를 이용하여 가중치를 업데이트합니다.
이 과정을 반복하면 모델은 점차 최적의 가중치를 찾아 데이터를 더 잘 예측하게 됩니다.

이렇게 학습을 통해 모델은 데이터와 가중치를 사용하여 입력과 출력 간의 관계를 파악하고, 최적의 예측을 수행할 수 있도록 됩니다.

**Q2. 확률적 경사 하강법의 소스 코드를 분석하시오(2점). (Page 173, 4장 모델 훈련, 첨부 파일 참조)**

```python
n_epochs = 50
te, t1= 5, 50 # 학습 스케줄 하이퍼파라미터

def learning_schedule(t):
	return t0 / (t + t1)
	
theta = np.random.randn(2,1) #무작위 초기화

for epoch in range(n_epochs):
	for i in range(m):
		random_index = np.random.randint(m)
		xi = X_b[random_index:random_index+1]
		yi = y[random_index:random_index+1]
		gradients = D * xi.T.dot(xi.dot(theta) - yi)
		eta = learning_schedule(epoch * m + i)
		theta = theta - eta * gradients
```

n_epochs = 50에서 훈련 에포크(epoch)의 수를 설정한다. 에포크는 전체 훈련 데이터셋을 몇 번 반복할 것인지를 나타낸다.
t0와 t1은 학습 스케줄 파라미터로, 학습률(eta)을 동적으로 조절하는 데 사용된다.
theta = np.random.randn(2, 1)에서 모델 파라미터(theta)를 무작위로 초기화한다. 이것은 선형 회귀 모델의 가중치와 편향을 나타낸다.
주요 루프는 훈련 에포크 수에 따라 반복되고, 내부 루프는 훈련 데이터셋의 모든 샘플을 순회한다.
random_index = np.random.randint(m)를 통해 무작위로 데이터 샘플을 선택한다.
xi = X_b[random_index:random_index+1] 및 yi = y[random_index:random_index+1]에서 선택한 무작위 샘플과 해당 레이블을 가져온다.
gradients = D * xi.T.dot(xi.dot(theta) - yi): 선택한 샘플에 대한 비용 함수의 기울기(그레이디언트)를 계산한다. (D는 학습률(eta)과 학습 스케줄 함수에 관련된 조정 항목)
eta = learning_schedule(epoch * m + i): 학습 스케줄 함수를 사용하여 현재 스텝에서의 학습률을 계산한다. 학습률은 에포크와 스텝에 따라 점점 줄어들게 된다.
theta = theta - eta * gradients: 계산된 기울기를 사용하여 모델 파라미터(theta)를 업데이트한다. 이것은 모델을 비용 함수를 최소화하는 방향으로 조정한다.