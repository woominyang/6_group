# note#6

2023.10.31

**Q1. 앙상블 학습의 핵심과 원리에 대해 궁금합니다.그리고 왜 다양한 예측기를 결합하는 것이 유용할까요?**

앙상블 학습은 다양한 예측기들을 결합하여 더 강력하고 안정적인 예측 모델을 만드는 기법입니다! 이 아이디어의 핵심은 다양성이라 생각합니다.
각 예측기는 데이터를 다양한 관점에서 해석하고 오분류하는 경향을 가집니다.
이러한 다양성은 모델의 편향은 비슷하지만 분산을 줄여주어 모델의 일반화 능력을 향상시킵니다.
다양한 예측기의 의견을 종합함으로써 개별 모델의 약점을 보완하고 보다 정확하고 신뢰성 있는 예측을 얻을 수 있습니다.

---

**Q2. 투표 기반 분류기에서 직접 투표와 간접 투표의 차이는 무엇일까요?**

투표 기반 분류기는 여러 예측기로부터 예측을 수집하여 가장 인기 있는 클래스를 선택하는 방식입니다.
직접 투표는 예측기들의 예측값을 다수결로 결정하고, 간접 투표는 예측기들의 예측 확률값을 평균하여 예측값을 결정합니다.
간접 투표는 예측기들이 예측한 확률값을 고려하기 때문에 직접 투표보다 더 좋은 성능을 보입니다.

---

**Q3. 배깅과 페이스팅의 차이점과 각각 어떤 상황에서 사용될 수 있나요?**

배깅과 페이스팅은 모두 데이터의 부분 집합을 사용하여 예측기를 학습하는 방법입니다.
배깅은 중복을 허용한 샘플링 방식으로, 같은 데이터 샘플이 여러 번 선택될 수 있습니다. 

반면, 페이스팅은 중복을 허용하지 않는 샘플링 방식입니다.
배깅은 훈련 데이터가 많을 때 사용하며 분산을 줄이고 안정성을 높이는 데 도움을 줍니다.
페이스팅은 데이터가 적을 때 사용하며 과대적합 위험을 줄이는 데 도움을 줍니다.

---

**Q4. 랜덤 포레스트는 왜 결정 트리의 앙상블로 사용되나요?**

랜덤 포레스트는 배깅을 적용한 결정 트리의 앙상블 모델입니다.
랜덤 포레스트에서는 다수의 결정 트리가 훈련되고, 각 트리의 노드 분할 시에 전체 특성 중에서 무작위로 선택한 특성을 사용합니다.
이렇게 하면 트리들이 서로 다양한 예측을 수행하며, 편향은 비슷하지만 분산을 줄이게 됩니다.
랜덤 포레스트는 과대적합을 줄이고 안정적인 예측을 제공하므로 결정 트리의 앙상블로 사용됩니다.

---

**Q5. 엑스트라 트리는 어떻게 작동하며, 일반적인 랜덤 포레스트와 어떤 차이가 있을까요?**

엑스트라 트리는 랜덤 포레스트의 변종으로, 특성과 특성 임곗값을 모두 무작위로 선택하는 방식으로 작동합니다.
이로 인해 엑스트라 트리는 일반적인 랜덤 포레스트보다 속도가 빠르지만 편향이 늘어나고 분산이 줄어듭니다.

---

**Q6. 에이다부스트와 그레이디언트 부스팅의 주요 차이점은 무엇인가요?**

에이다부스트는 이전 모델에서 잘못 분류된 샘플에 가중치를 더 주어 새로운 모델을 생성하는 방식으로 작동합니다.
그레이디언트 부스팅은 이전 예측기의 잔여 오차를 학습하여 새로운 모델을 생성하는 방식으로 작동합니다.

---

**Q7. 그레이디언트 부스팅에서 학습률과 결정 트리 수를 조절하여 높였을 때와 낮췄을 때의 영향에 대해 궁금합니다!**

그레이디언트 부스팅에서 학습률을 조절하여 모델의 성능을 향상시킬 수 있습니다.
학습률을 높였을 때는 학습률을 높이면 각 예측기가 빠르게 학습하게 되며, 훈련 과정이 빨라집니다.
그러나 이 경우 모델이 과대적합(overfitting)될 가능성이 높아지며, 높은 분산을 가질 수 있습니다.
학습률을 낮췄을 때는 학습률을 낮추면 각 예측기가 더 조금씩 학습하게 되며, 모델이 더 안정적으로 수렴합니다.
이는 일반화 능력을 향상시키지만 모델의 훈련 시간이 늘어날 수 있습니다. 낮은 학습률은 과대적합을 줄이는 데 도움을 줄 수 있습니다.

---

**Q8. 확률적 그레이디언트 부스팅은 어떤 방식으로 작동하며 어떤 이점을 제공하나요?**

확률적 그레이디언트 부스팅은 그레이디언트 부스팅의 변종으로, 모든 훈련 데이터를 사용하는 대신 일부 훈련 데이터만 무작위로 선택하여 사용하는 방식으로 작동합니다.
동작 원리는 각 트리 학습 단계에서 전체 훈련 데이터 중 일부를 무작위로 선택하여 사용합니다.
이로써 각 결정 트리는 다른 데이터 하위 집합을 사용하며, 이것이 모델의 무작위성을 증가시킵니다.
확률적 그레이디언트 부스팅의 이점으로는
1.모델 학습 속도 향상: 전체 데이터 대신 일부 데이터만 사용하므로 학습 속도가 향상됩니다.
2.과대적합 감소: 무작위성을 더 많이 주입하여 모델의 과대적합 가능성을 감소시킵니다.
3.일반화 능력 향상: 모델의 일반화 능력을 향상시키는 결과를 낼 수 있습니다.

---

**Q9. 스태킹은 어떻게 작동하나요?**

스태킹은 여러 예측기의 예측값을 입력으로 사용하여 최종 예측 모델을 학습시키는 방법입니다.
기본 예측기들을 첫 번째 레이어로 구성하고, 그 결과를 사용하여 최종 예측 모델을 학습합니다.
스태킹은 다양한 예측기를 결합하여 더 강력한 예측 모델을 생성하는 데 사용됩니다.

---

**Q10. 오차 분석은 어떻게 모델을 개선하는데 활용되나요?**

오차 분석은 모델의 예측 오차를 분석하여 어떤 클래스를 잘못 분류했는지, 왜 그런 오차가 발생했는지를 이해하는 방법입니다.
이를 통해 모델을 개선하기 위한 조치를 취할 수 있습니다.

예를들면 오분류된 샘플에 대한 추가 학습이나 특성 엔지니어링을 수행할 수 있습니다.