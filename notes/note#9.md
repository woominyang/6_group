# note#9

[2023.11.21 - 10장 케라스를 사용한 인공 신경망]

[생물학적 뉴런]

**Q1. 생물학적 뉴런과 인공 뉴런의 기본 구조와 기능에는 어떤 유사점과 차이점이 있고, 이러한 차이가 인공 신경망의 설계와 작동에 어떤 영향을 미칠까요?**

A1. 생물학적 뉴런은 신호를 받고 처리하며, 다른 뉴런으로 전달하는 세포입니다.
이들은 수상돌기로 신호를 받고 축삭으로 신호를 전달합니다.
반면, 인공 뉴런은 이를 수학적으로 모델링한 것으로 입력 신호에 가중치를 곱하고 합산 후 활성화 함수를 거쳐 출력합니다.
유사점은 둘 다 입력을 받고 출력을 내는 구조를 가지고 있다는 점입니다.
차이점은 생물학적 뉴런이 복잡한 생화학적 과정을 거치는 반면에 인공 뉴런은 간소화된 수학적 모델을 사용한다는 점입니다.
이 차이는 인공 신경망을 설계할 때 유연성을 주지만 생물학적 뉴런의 모든 기능을 모방하지는 못합니다.

**Q2.인공 뉴런으로 논리 연산을 어떻게 할 수 있을까요? 예시와 그 연산이 요즘 딥러닝 모델에서 어떻게 쓰이는지도 궁금합니다!**

A2. 인공 뉴런은 간단한 논리 연산, 예를 들어 AND, OR, NOT 연산을 수행할 수 있습니다.
예를 들어, AND 연산을 수행하는 인공 뉴런은 두 입력 모두 높은 값을 가질 때만 높은 출력을 내도록 가중치와 활성화 함수를 설정합니다.
이런 기본 연산들은 복잡한 딥러닝 모델에서 기본적인 구성 요소로 사용됩니다.
예를 들어, CNN 에서는 이러한 뉴런들이 이미지의 특정 패턴을 인식하는데 사용됩니다.
각 뉴런은 이미지의 특정 부분에 반응하여 복잡한 패턴과 구조를 학습하는 데 기여합니다.

---

[퍼셉트론]

**Q3. 퍼셉트론의 주요 약점은 무엇이며, 이러한 약점들이 후속 연구 및 발전에 어떤 영향을 미쳤나요?**

A3.퍼셉트론의 가장 큰 약점 중 하나는 XOR 문제와 같은 비선형 문제를 해결할 수 없다는 것입니다.
이 약점은 인공 신경망 연구에 큰 타격을 주었으며 동시에 후속 연구의 방향을 제시했습니다.
이로 인해 다층 퍼셉트론과 같은 복잡한 네트워크 구조와 역전파 알고리즘의 개발로 이어졌고 이는 현대 딥러닝의 기초를 마련했습니다.

**Q4.퍼셉트론 학습 알고리즘이 어떻게 작동하고, 이 방법이 현재 딥러닝 알고리즘과 어떻게 다른가요?**

A4. 퍼셉트론 학습 알고리즘은 입력에 가중치를 곱하고, 합산한 후 활성화 함수를 통과시켜 결과를 도출합니다.
학습 과정에서는 잘못된 예측에 대해 가중치를 조정합니다.
현재 딥러닝 알고리즘과의 차이점은 딥러닝은 보통 여러 층을 가지며 각 층은 복잡한 변환을 수행합니다.
또한, 역전파와 같은 고급 학습 기법을 사용하여 더 정교하게 가중치를 조정합니다.
딥러닝은 또한 비선형 활성화 함수를 사용해 비선형 문제도 해결할 수 있습니다.

---

[다층 퍼셉트론과 역전파]

**Q5. 역전파 알고리즘이 다층 퍼셉트론 학습에 어떻게 적용되고,이 알고리즘이 해결하는 주요 문제는 무엇일까요?**

A5. 역전파 알고리즘은 네트워크의 출력 오류를 이용하여 각 뉴런의 가중치를 조정하는 방법입니다.
이는 네트워크의 마지막 층에서 시작하여 오류를 거꾸로 전파시키면서 각 층의 가중치를 수정합니다.
이 알고리즘은 다층 퍼셉트론이 복잡한 패턴을 학습할 수 있게 해줍니다.
특히, 역전파는 네트워크가 깊어질 때 중요해지며 이전에는 해결하기 어려웠던 깊은 네트워크의 효율적인 학습 문제를 해결해줍니다.

**Q6. 다층 퍼셉트론에서 활성화 함수가 왜 중요하고, 비선형 활성화 함수가 필요한 이유는 뭘까요?**

A6. 활성화 함수는 신경망에서 입력 신호의 합을 출력 신호로 변환하는 역할을 합니다.
비선형 활성화 함수는 신경망에 비선형성을 도입하여 네트워크가 더 복잡한 패턴과 함수를 학습할 수 있게 합니다.
만약 활성화 함수가 선형이라면 신경망의 여러 층을 결합해도 결국 하나의 선형 함수를 형성하게 되어 복잡한 문제를 해결하는 데 한계가 있습니다.
비선형 활성화 함수는 이를 해결하고 신경망이 더 강력해지게 만듭니다.

---

[회귀를 위한 다층 퍼셉트론]

**Q7. 회귀 문제를 풀 때 다층 퍼셉트론에서 출력 뉴런 수와 활성화 함수 선택이 왜 중요한가요?**

A7. 출력 뉴런의 수는 회귀 문제의 출력 차원을 결정합니다.
예를 들어, 주택 가격 예측에서는 하나의 출력 뉴런이 필요하고 주식의 가격과 거래량을 예측하려면 두 개의 출력 뉴런이 필요합니다. 활성화 함수는 출력 값을 조절하는 역할을 합니다.
회귀 문제에서는 보통 선형 활성화 함수(예: 항등 함수)를 사용합니다.
이는 출력이 특정 범위에 제한되지 않고, 다양한 실수 값을 가질 수 있게 해줍니다.

**Q8. 회귀 문제에서 사용되는 다양한 손실 함수에는 무엇이 있고, 이들을 선택할 때 어떤 기준을 고려해야 할까요?**

A8. 회귀 문제에서 일반적으로 사용되는 손실 함수에는 평균 제곱 오차(MSE), 평균 절대 오차(MAE), 허브러 손실(Huber loss) 등이 있습니다.
MSE는 실제 값과 예측 값의 차이의 제곱을 평균내는 것으로 큰 오차에 더 많은 패널티를 줍니다.
MAE는 오차의 절대값의 평균을 계산하며 이상치에 덜 민감합니다.
허브러 손실은 MSE와 MAE의 중간 형태로 작은 오차에는 제곱을 큰 오차에는 절대값을 사용합니다.
손실 함수 선택 시 데이터의 특성과 이상치의 존재, 성능 목표 등을 고려해야 합니다.

---

[분류를 위한 다층 퍼셉트론]

**Q9. 이진 분류와 다중 클래스 분류를 수행할 때 다층 퍼셉트론의 구조적 차이와 각각에 적합한 활성화 함수는 무엇인가요?**

A9.이진 분류에서는 보통 하나의 출력 뉴런과 시그모이드 활성화 함수를 사용합니다.
이 구조는 결과가 0과 1 사이의 확률로 표현되게 합니다.
다중 클래스 분류에서는 클래스 수에 해당하는 출력 뉴런과 소프트맥스 활성화 함수를 사용합니다.
소프트맥스는 모든 출력 뉴런의 값을 확률 분포로 변환하여 각 클래스에 속할 확률을 나타냅니다.

**Q10. 분류 문제에서 크로스 엔트로피 손실 함수의 역할과 중요성에 대해 설명해주세요.**

A10. 크로스 엔트로피 손실 함수는 실제 레이블과 예측된 확률 분포 간의 차이를 측정합니다.
이 함수는 정답에 해당하는 클래스의 예측 확률에 집중하여 이 확률이 낮을수록 더 큰 손실을 반환합니다.
이로 인해 모델이 정확한 분류를 위해 정답 클래스의 확률을 높이도록 학습하게 됩니다.
크로스 엔트로피는 특히 분류 문제에서 모델의 성능을 높이는 데 중요한 역할을 합니다.

---

[케라스로 다층 퍼셉트론 구현하기]

**Q11. 케라스의 시퀀셜 API를 사용해서 간단한 다층 퍼셉트론을 어떻게 만들 수 있나요? 그 과정에서 어떤 결정들이 중요한가요?**

A11. 케라스에서 다층 퍼셉트론을 만드는 첫 단계는 Sequential 모델을 생성하는 것입니다.
이 모델은 레이어를 순차적으로 쌓을 수 있게 해줍니다. 처음에는 입력 데이터에 맞는 Dense 레이어를 추가하시면 됩니다.
그 후에 여러 개의 은닉층을 추가하는데 이때 각 레이어에 뉴런의 수와 활성화 함수를 어떻게 설정할지 결정해야 합니다.
마지막으로 출력 레이어를 추가하면 되는데 이것은 문제의 유형에 맞게 설정해야 합니다.
그리고 나서 모델을 컴파일합니다. 이때 손실 함수, 옵티마이저, 평가 지표를 정해줘야 합니다.
마지막으로 fit 함수를 이용해 모델을 학습시키면 됩니다.

중요한 결정들로는 레이어의 수, 각 레이어에 배치될 뉴런의 수, 활성화 함수의 종류, 손실 함수와 옵티마이저의 선택 등이 있습니다.
이들은 모두 모델의 성능에 큰 영향을 미칠 수 있으니 신중하게 결정해야 합니다.

**Q12. 케라스를 사용한 이미지 분류에서 데이터 전처리의 중요성과 주의할 점들에 대해 설명해주세요.**

A12. 이미지 분류 문제에서 데이터 전처리는 매우 중요합니다.
먼저 이미지 데이터의 픽셀 값을 0과 1 사이로 정규화하는 스케일링이 필요합니다.
이 과정은 학습의 안정성과 효율성을 높이는 데 도움을 줍니다. 또한 모든 이미지를 같은 크기로 조정해야 합니다.
데이터 증강을 통해 학습 데이터의 다양성을 높일 수 있습니다. 이는 회전, 확대, 이동 같은 기법들을 사용해서 할 수 있습니다.
마지막으로 색상 처리도 중요한 부분입니다. 필요에 따라 컬러 이미지를 흑백으로 바꾸거나 반대로 할 수 있습니다.

---

[함수형 API를 사용해 복잡한 모델 만들기]

**Q13. 케라스의 함수형 API를 사용하는 주된 이유와 이를 통해 구현할 수 있는 신경망의 예시를 들어 설명해주실 수 있나요?**

A13. 케라스의 함수형 API를 사용하는 주된 이유는 더 복잡하고 유연한 모델 아키텍처를 구현할 수 있기 때문입니다.
다중 입력이나 출력, 공유 레이어, 비순차적 데이터 흐름이 필요한 경우에 매우 유용합니다.
예를 들면, 텍스트와 이미지 데이터를 동시에 처리하는 모델을 만들 수 있습니다.
이런 모델에서는 각 입력 유형에 대해 별도의 처리 경로를 정의하고 이를 하나의 모델로 통합할 수 있습니다.

**Q14. 와이드&딥 신경망이 어떻게 생겼고, 왜 그런 구조가 유용하다고 생각하나요?**

A14. 와이드&딥 신경망은 한쪽은 와이드(넓은) 경로를, 다른 한쪽은 딥(깊은) 경로를 가지는 구조입니다.
와이드 경로는 간단한 규칙과 패턴을 학습하는 데 유용하고 딥 경로는 보다 복잡한 문제를 해결하는 데 도움을 줍니다.
이런 구조는 모델이 다양한 종류의 데이터 패턴을 더 잘 이해하고 학습할 수 있게 해줘서 유용합니다.

---

[신경망 하이퍼파라미터 튜닝하기]

**Q15. 신경망에서 하이퍼파라미터 튜닝이 왜 중요하나요? 특히 은닉층 개수와 뉴런 수가 모델 성능에 미치는 영향에 대해 궁금합니다.**

A15. 하이퍼파라미터 튜닝은 신경망의 성능을 최적화하는 데 매우 중요합니다.
은닉층의 수와 뉴런의 수는 모델이 얼마나 많은 정보를 학습할 수 있는지를 결정합니다.
적절한 수의 은닉층과 뉴런을 설정하면 복잡한 패턴을 잘 학습할 수 있지만 너무 많으면 과적합의 위험이 있습니다.

**Q16. 다양한 하이퍼파라미터 튜닝 기법들을 실제 활용할 때 고려해야 할 요소들은 무엇일까요?**

A16. 하이퍼파라미터를 튜닝할 때는 학습 데이터의 크기, 모델의 복잡성, 사용 가능한 계산 자원, 과적합의 가능성 등을 고려해야 합니다.
또한 그리드 탐색, 랜덤 탐색, 베이지안 최적화와 같은 다양한 튜닝 기법을 실험해보면서 각각의 장단점을 이해하고 최적의 설정을 찾아야 합니다.