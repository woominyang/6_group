# note#10

**Q1. 합성곱 층에서 필터 크기의 변화가 결과에 미치는 영향은 무엇인가요?**
A1. 필터 크기는 감지되는 특징의 범위를 결정합니다.
작은 필터는 더 세밀한 패턴을 감지하는 반면, 큰 필터는 더 넓은 범위의 특징을 감지합니다.
필터 크기가 결과에 미치는 영향은 문제의 특성과 데이터의 성질에 따라 다릅니다.

**Q1. 합성곱 층의 필터 크기를 변경하면 결과에 어떤 영향을 미치나요?**
A1. 필터 크기는 합성곱 중 감지된 특징의 범위를 결정하는 역할을 하는데, 작은 필터는 복잡한 패턴을 포착하는 데 능숙하고, 큰 필터는 더 넓은 범위의 특징을 포착하는 데 능숙하다는 장점이 있습니다.

---

**Q2. 여러 합성곱 층을 쌓는 이유와 각 층이 수행하는 역할에 대해 설명해주세요.**
A2. 여러 합성곱 층을 쌓는 것은 네트워크가 더 복잡하고 추상적인 특징을 학습하도록 돕습니다.
초기 층은 간단한 특징을 감지하고, 더 깊은 층으로 갈수록 복잡한 특징을 학습합니다.

**Q2. 더 복잡하고 추상적인 특징이란 무엇입니까?**
A2. 이미지 인식을 예로 들어 더 복잡하고 추상적인 특징은 복잡한 패턴이나 질감 또는 관련 객체의 다양한 조합들을 말할 수 있습니다. 다양한 합성곱 층을 사용하여 모델은 복잡한 특징을 인식하고 표현하는 방법을 개선할 수 있고, 이는 입력된 데이터를 더욱 섬세하고 정밀하게 이해할 수 있도록 합니다.

---

**Q3. 합성곱 신경망에서 드롭아웃 층의 역할과 중요성에 대해 설명해주세요.**
A3. 드롭아웃 층은 모델의 과대적합을 방지하기 위해 사용됩니다.
학습 과정에서 일부 뉴런을 임의로 비활성화시켜, 네트워크가 특정 뉴런에 지나치게 의존하는 것을 방지합니다.

**Q3. 합성곱 신경망에서 드롭아웃은 어떻게 구현될까요?**
A3. 훈련 반복 중 뉴런을 비활성화 하여 노이즈를 발생시킵니다. 발생된 노이즈를 통해 특정 뉴런에 민감하게 반응하는 것을 줄일 수 있습니다. 결과적으로 특정 뉴런에 지나치게 의존하지 않고 데이터의 더 넓은 범위의 패턴에 적응하여 과적합을 줄입니다.

---

**Q4. CNN에서 풀링 층의 역할은 무엇이며 최대 풀링과 평균 풀링의 차이점은 무엇인가요?**
A4. 풀링 층은 특성 맵의 공간적 크기를 줄이고 중요한 정보를 유지하는 역할을 합니다.
최대 풀링은 영역 내 최대값을 선택하는 반면, 평균 풀링은 평균값을 사용합니다.

**Q4. 최대 풀링과 평균 풀링을 어떤 때에 선택하여 사용하나요?**
A4. 각 풀링을 선택하는것에는 다양한 이유가 있습니다. 이동 불변성 측면에서 최대 풀링은 위치가 변할 때 가장 큰 값을 선택하기 때문에 정확한 위치에 상대적인 불변성을 띄지만, 평균 풀링은 작은 위치 변화에 덜 민감하게 반응하여 불변성을 부여합니다. 노이즈 측면에서 최대 풀링은 더 높은 활성값을 가진 뉴런을 선택하여 작은 노이즈에 덜 민감하고, 평균 풀링은 모든 값의 평균을 취하여 노이즈에 상대적으로 덜 민감합니다.

---

**Q5. AlexNet과 VGGNet의 구조적 차이와 각각의 네트워크가 가져온 혁신은 무엇인가요?**
A5. AlexNet은 깊이와 너비가 더 큰 구조로 LeNet-5를 확장한 것이며, 드롭아웃과 데이터 증식을 도입했습니다.
VGGNet은 모든 합성곱 층에서 3x3 필터만을 사용함으로써 네트워크의 깊이를 증가시켰습니다.

**Q5. AlexNet과 VGGNet은 어떨 때 사용하는 것이 좋은가요?**

A5. AlexNet은 대규모 데이터셋에서 이미지 분류 작업을 할 때 사용하면 좋습니다. 대규모 데이터셋에서 복잡한 특성을 학습하는 데 좋은 성능을 보여, 높은 해상도의 이미지에 유리합니다. VGGNet 또한 이미지 등 비전 작업에 효과적으로 사용할 수 있고, 중간 깊이의 네트워크 구조를 가져 비전 작업에서 좋은 일반화 성능을 보이기 때문에 전이 학습에도 자주 사용됩니다.

---

**Q6. ResNet이 도입한 '잔차 학습' 개념은 어떻게 깊은 신경망의 학습을 개선시키나요?**
A6. 잔차 학습은 입력을 네트워크의 깊은 층으로 바로 전달하는 스킵 연결을 통해 깊은 신경망의 학습을 용이하게 합니다.
이는 소실된 기울기 문제를 완화하여 더 깊은 네트워크를 효과적으로 학습시킬 수 있게 합니다.

**Q6. ResNet이 소실된 기울기 문제를 완화하는 방법은 무엇인가요?**
A6. 신경망에서 역전파로 인해 기울기가 작아지거나 소실되는 문제가 있는데 ResNet에서는 각 층에서 입력 데이터를 출력에 직접 추가하는 형태로 레지듀얼 블록을 구성하게 됩니다. 기존 신경망의 출력을 F(x), x를 입력으로 둘 때, H(x) = F(x) + x형태로 표현할 수 있습니다. 이를 통해 기울기가 신경망을 지날 때 일부 기울기는 소실되지 않고 전달됨으로써 기울기 소실 문제를 완화할 수 있습니다.

---

**Q7. GoogLeNet의 인셉션 모듈은 어떤 구조이며, 이 구조가 가져오는 이점은 무엇인가요?**
A7. 인셉션 모듈은 다양한 크기의 필터와 풀링을 동시에 적용하여 다양한 스케일의 특징을 한 번에 감지할 수 있도록 설계되었습니다.
이는 모델의 효율성을 높이고, 파라미터 수를 줄이는 데 기여합니다.

**Q7. 인셉션 모듈은 어떻게 동작하나요?**
A7. 네트워크에 1x1, 3x3, 5x5 합성곱 필터 및 최대 풀링을 동시에 적용하고 결과를 채널 방향으로 합쳐 다양한 크기의 특징을 추출합니다. 1x1 합성곱은 채널 수를 조절하여 상호작용을 증가시키고, 3x3, 5x5 합성곱은 더 넓은 영역의 특징을 감지합니다. 이를 통해 인셉션 모듈은 다양한 크기와 특징을 학습하고 결합해 신경망의 성능을 높일 수 있습니다.

---

**Q8. Xception 아키텍처가 기존의 CNN 모델과 다른 점은 무엇이며, 이를 통해 어떤 성능 개선을 이루었나요?**
A8. Xception은 깊이별 분리 합성곱을 사용하여 공간적 특징과 채널 간의 특징을 분리하여 학습합니다.
이는 모델의 효율성과 성능을 향상시키는 데 도움을 줍니다.

**Q8. Xception 의 깊이별 분리 합성곱에 대해 설명해주세요**
A8. 깊이별 분리 합성곱은 입력 데이터의 각 채널에 대해 따로따로 커널을 적용하고, 이를 통해 공간적인 특징을 학습합니다. 먼저, 입력 데이터의 깊이별로 분리된 커널을 사용하여 채널별로 특징을 추출하고, 이후에 1x1 합성곱을 통해 채널 간 상호작용을 학습합니다. 이러한 구성은 기존의 합성곱에 비해 계산 비용을 획기적으로 줄이면서도 효과적으로 특징을 학습하며, 모델의 경량화와 성능 향상에 기여합니다. Xception은 이 깊이별 분리 합성곱을 통해 높은 성능을 달성하면서도 효율적인 모델 구조를 구현합니다.

---

**Q9. SENet 모델의 SE 블록이 기존 CNN 구조에 어떤 개선을 가져왔나요?**
A9. SE 블록은 특성 맵의 채널별 중요도를 학습하여 채널별 가중치를 조정합니다.
이를 통해 모델은 보다 중요한 특징에 더 많은 주의를 기울일 수 있습니다.

**Q9. 채널별 가중치의 변화는 무엇을 의미하나요?**
A9. 신경망이 학습하는 동안 각 채널의 특성을 평가하고, 중요한 채널에 높은 가중치를 부여해 중요도를 강조하거나, 중요하지 않은 채널에 낮은 가중치를 부여해 영향을 감소시킬 수 있습니다.

---

**Q10. MobileNet과 같은 경량 신경망이 중요한 이유와 이들이 어떻게 효율적인 계산을 달성하는지 설명해주세요.**
A10. 경량 신경망은 제한된 컴퓨팅 자원을 가진 모바일 및 임베디드 시스템에서 사용됩니다.
MobileNet 같은 모델은 깊이별 분리 합성곱을 사용하여 모델의 크기와 계산 복잡성을 크게 줄입니다.

**Q10. 모바일 또는 임베디드 시스템을 위해 모델을 경량화 하는 방법에는 어떤 것들이 있나요?**
A10. 불필요한 가중치를 제거하거나 없에 모델의 크기를 줄이고, 가중치 중요도에 따라 모델을 재조정 및 훈련하여 모델의 경량화를 이룰 수 있습니다. 또한 모든 채널에 대해 각각의 합성곱을 수행하는 깊이별 분리 합성곱을 사용하여 모델의 계산량을 줄이고, 모델의 파라피터 수를 최소화 하거나 양자화를 통해 메모리 사용량을 줄여 계산량을 감소시키는 등의 방법을 통해 모델을 경량화 할 수 있습니다.